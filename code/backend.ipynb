{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installation of necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade jupyter notebook transformers faiss-cpu sentence-transformers langchain openai tiktoken PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import faiss\n",
    "import transformers\n",
    "import sentence_transformers\n",
    "import fitz\n",
    "import re\n",
    "import warnings\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File path to the PDF file containing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/tihum_kabir/Law-Enforcement/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File Processing | reading and loading files |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and process all PDF files in the directory\n",
    "def load_and_process_all_pdfs(directory):\n",
    "    pdf_texts = []\n",
    "    \n",
    "    # List all files in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".pdf\"):  # Check if the file is a PDF\n",
    "            file_path = os.path.join(directory, file_name)  # Get the full path\n",
    "            print(f\"Processing {file_path}...\")  # Display the current file being processed\n",
    "            \n",
    "            # Extract text from the PDF\n",
    "            doc = fitz.open(file_path)  # Open the PDF file\n",
    "            file_text = [page.get_text(\"text\") for page in doc]  # Extract text from all pages\n",
    "            \n",
    "            pdf_texts.append((file_name, file_text)) \n",
    "    \n",
    "    return pdf_texts\n",
    "\n",
    "# Function to process extracted PDF text into sections\n",
    "def process_pdf_sections(pdf_texts):\n",
    "    sections = []  # List to store the structured data (chapter, section, content)\n",
    "    current_chapter, current_section, content = None, None, []  \n",
    "\n",
    "    # Process the extracted text into sections\n",
    "    for file_name, pdf_text in pdf_texts:\n",
    "        for page_text in pdf_text:\n",
    "            lines = page_text.splitlines()  # Split the page text into lines\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()  # Remove leading/trailing whitespaces  \n",
    "\n",
    "                # Regex to identify chapter headings\n",
    "                if re.match(r\"^CHAPTER\\s+[IVXLCDM]+\", line):\n",
    "                    if current_section:\n",
    "                        sections.append({\"chapter\": current_chapter, \"section\": current_section, \"content\": \"\\n\".join(content)})\n",
    "                        content = []  # Reset content\n",
    "                    current_chapter = line\n",
    "                    current_section = None  # Reset section\n",
    "                \n",
    "                # Regex to identify section headings\n",
    "                elif re.match(r\"^\\d+\\.\", line):\n",
    "                    if current_section:\n",
    "                        sections.append({\"chapter\": current_chapter, \"section\": current_section, \"content\": \"\\n\".join(content)})\n",
    "                        content = []  # Reset content\n",
    "                    current_section = line\n",
    "\n",
    "                # Add content to the current section\n",
    "                else:\n",
    "                    content.append(line)\n",
    "\n",
    "    # Save the last section\n",
    "    if current_section:\n",
    "        sections.append({\"chapter\": current_chapter, \"section\": current_section, \"content\": \"\\n\".join(content)})\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Load and process all PDFs in the directory\n",
    "pdf_texts = load_and_process_all_pdfs(directory)\n",
    "\n",
    "# Print the first 500 characters of the text from each PDF file for verification\n",
    "for file_name, text in pdf_texts:\n",
    "    print(f\"First 500 characters from {file_name}:\\n{text[0][:500]}\\n\")\n",
    "\n",
    "# Process the extracted PDF text into sections\n",
    "sections = process_pdf_sections(pdf_texts)\n",
    "\n",
    "# Print the first two sections for verification\n",
    "for sec in sections[:2]: \n",
    "    print(f\"Chapter: {sec['chapter']}\\nSection: {sec['section']}\\nContent: {sec['content'][:300]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Loading (Mistral 7B Instruct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login   # Hugging Face Hub login\n",
    "token = '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'    # Replace with your own token\n",
    "login(token)    # Login to the Hugging Face Hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"  \n",
    "\n",
    "# BitsAndBytesConfig for 4-bit quantization (reduces memory usage)\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Load Mistral model with 4-bit quantization in CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence Transformer and FAISS Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Sentence Transformer model\n",
    "sentence_transformer_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Create embeddings for each section\n",
    "section_texts = [sec['content'] for sec in sections]\n",
    "embeddings = sentence_transformer_model.encode(section_texts, convert_to_tensor=True)  # Create embeddings\n",
    "\n",
    "# Convert embeddings to numpy array for FAISS\n",
    "embeddings_np = embeddings.cpu().detach().numpy()   # Move to CPU\n",
    "\n",
    "# Build a FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # Use L2 distance for similarity\n",
    "index.add(embeddings_np)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant sections based on a query\n",
    "def get_relevant_sections(query, k=3):  # k is the number of top similar sections to retrieve\n",
    "    query_embedding = sentence_transformer_model.encode([query], convert_to_tensor=True).cpu().detach().numpy()  # Embed the query\n",
    "\n",
    "    D, I = index.search(query_embedding, k=k)  # Search the FAISS index\n",
    "\n",
    "    retrieved_sections = [sections[i] for i in I[0]]  # Get the sections corresponding to the indices\n",
    "\n",
    "    context = \"\\n\".join([sec['content'] for sec in retrieved_sections])  # Combine the content of retrieved sections\n",
    "    return context\n",
    "# Test the retrieval functionality\n",
    "query = \"What is the punishment for murder?\"\n",
    "relevant_sections = get_relevant_sections(query)\n",
    "print(f\"Relevant Sections:\\n{relevant_sections}\")  # Check which sections were retrieved\n",
    "\n",
    "\n",
    "# Function to generate response using the Mistral model\n",
    "def generate_response(query):\n",
    "    context = get_relevant_sections(query)\n",
    "\n",
    "    input_text = f\"Context: {context}\\nQuery: {query}\\nResponse:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Explicitly set pad_token_id to eos_token_id to remove the warning\n",
    "    output = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process to clean the output\n",
    "    response = re.sub(r\"(?s).*Response:\", \"\", response).strip()  # Remove everything before and including \"Response:\"\n",
    "    response = re.sub(r\"\\n+\", \" \", response).strip()  # Remove excessive newlines or extra spaces\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fix and Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\") # Ignore warnings\n",
    "\n",
    "# Configure the quantization\n",
    "bnb_config.bnb_4bit_compute_dtype = torch.float16  # Ensures computation matches input dtype\n",
    "\n",
    "# Function to generate a response using the Mistral model\n",
    "def generate_response(query):\n",
    "    context = get_relevant_sections(query)  # Get the relevant context for the query\n",
    "    # Construct the input text\n",
    "    input_text = f\"Context: {context}\\nQuery: {query}\\nResponse:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate the output from the model\n",
    "    output = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the response:\n",
    "    response = re.sub(r\"(?s).*Response:\", \"\", response).strip()\n",
    "    response = re.sub(r\"\\n+\", \" \", response).strip()\n",
    "    response = re.sub(r\"(Context:.*|Query:.*)\", \"\", response).strip()\n",
    "    final_response = f\"\\nQuery: {query}\\nResponse: {response}\\n\"\n",
    "\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is the punishment for murder?\"\n",
    "response = generate_response(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is income tax law?\"\n",
    "response = generate_response(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is the punishment for theft?\"\n",
    "response = generate_response(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
